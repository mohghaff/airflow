[2023-09-05T12:23:09.783+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: postgres_hooks_v08.postgres_to_s3 scheduled__2022-05-02T00:00:00+00:00 [queued]>
[2023-09-05T12:23:09.817+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: postgres_hooks_v08.postgres_to_s3 scheduled__2022-05-02T00:00:00+00:00 [queued]>
[2023-09-05T12:23:09.818+0000] {taskinstance.py:1361} INFO - Starting attempt 5 of 6
[2023-09-05T12:23:09.877+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): postgres_to_s3> on 2022-05-02 00:00:00+00:00
[2023-09-05T12:23:09.902+0000] {standard_task_runner.py:57} INFO - Started process 30552 to run task
[2023-09-05T12:23:09.922+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'postgres_hooks_v08', 'postgres_to_s3', 'scheduled__2022-05-02T00:00:00+00:00', '--job-id', '4409', '--raw', '--subdir', 'DAGS_FOLDER/dags_with_postgres_hooks.py', '--cfg-path', '/tmp/tmpbpb0gx4a']
[2023-09-05T12:23:09.928+0000] {standard_task_runner.py:85} INFO - Job 4409: Subtask postgres_to_s3
[2023-09-05T12:23:10.202+0000] {task_command.py:415} INFO - Running <TaskInstance: postgres_hooks_v08.postgres_to_s3 scheduled__2022-05-02T00:00:00+00:00 [running]> on host 55ba52c92856
[2023-09-05T12:23:10.476+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='mohghaff' AIRFLOW_CTX_DAG_ID='postgres_hooks_v08' AIRFLOW_CTX_TASK_ID='postgres_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2022-05-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='5' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2022-05-02T00:00:00+00:00'
[2023-09-05T12:23:10.508+0000] {logging_mixin.py:151} WARNING - /home/***/.local/lib/python3.8/site-packages/***/utils/context.py:206 AirflowContextDeprecationWarning: Accessing 'next_ds_nodash' from the template is deprecated and will be removed in a future version. Please use '{{ data_interval_end | ds_nodash }}' instead.
[2023-09-05T12:23:10.524+0000] {base.py:73} INFO - Using connection ID 'postgres_localhost' for task execution.
[2023-09-05T12:23:10.594+0000] {dags_with_postgres_hooks.py:29} INFO - Saved orders file as dags/get_orders_20220502.csv
[2023-09-05T12:23:10.621+0000] {base.py:73} INFO - Using connection ID 'minio_conn' for task execution.
[2023-09-05T12:23:10.622+0000] {connection_wrapper.py:340} INFO - AWS Connection (conn_id='minio_conn', conn_type='aws') credentials retrieved from login and password.
[2023-09-05T12:23:12.119+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/boto3/s3/transfer.py", line 292, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.8/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
  File "/home/airflow/.local/lib/python3.8/site-packages/s3transfer/futures.py", line 266, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/s3transfer/tasks.py", line 139, in __call__
    return self._execute_main(kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/s3transfer/tasks.py", line 162, in _execute_main
    return_value = self._main(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/s3transfer/upload.py", line 758, in _main
    client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/botocore/client.py", line 535, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/botocore/client.py", line 980, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.exceptions.ClientError: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The Access Key Id you provided does not exist in our records.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/dags_with_postgres_hooks.py", line 32, in postgres_to_s3
    s3_hook.load_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/amazon/aws/hooks/s3.py", line 138, in wrapper
    return func(*bound_args.args, **bound_args.kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/amazon/aws/hooks/s3.py", line 92, in wrapper
    return func(*bound_args.args, **bound_args.kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/amazon/aws/hooks/s3.py", line 1074, in load_file
    client.upload_file(filename, bucket_name, key, ExtraArgs=extra_args, Config=self.transfer_config)
  File "/home/airflow/.local/lib/python3.8/site-packages/boto3/s3/inject.py", line 143, in upload_file
    return transfer.upload_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/boto3/s3/transfer.py", line 298, in upload_file
    raise S3UploadFailedError(
boto3.exceptions.S3UploadFailedError: Failed to upload dags/get_orders_20220502.csv to ***/orders/20220502.txt: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The Access Key Id you provided does not exist in our records.
[2023-09-05T12:23:12.133+0000] {taskinstance.py:1400} INFO - Marking task as UP_FOR_RETRY. dag_id=postgres_hooks_v08, task_id=postgres_to_s3, execution_date=20220502T000000, start_date=20230905T122309, end_date=20230905T122312
[2023-09-05T12:23:12.151+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 4409 for task postgres_to_s3 (Failed to upload dags/get_orders_20220502.csv to ***/orders/20220502.txt: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The Access Key Id you provided does not exist in our records.; 30552)
[2023-09-05T12:23:12.179+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-09-05T12:23:12.203+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
